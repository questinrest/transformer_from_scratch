{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3352c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6f7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to define first\n",
    "\n",
    "# X is an input matrix, having shape (seq_len, dmodel)\n",
    "# Wq is query trainable, having shape (dmodel, dk)\n",
    "# Wk is key trainable, having shape (dmodel, dk)\n",
    "# Wv is value trainable, having shape (dmodel, dk), dv = dk\n",
    "# Wo trainable, having shape (dk*h, dmodel)\n",
    "# Q = X @ Wq, shape (seq_len, dk)\n",
    "# K = X @ Wk, shape (seq_len, dk)\n",
    "# V  = X @ Wv, shape (seq_len, dk), dk= dv\n",
    "# A = Attention(Q, K.t()), shape (seq_len, seq_len) where Attention(Q, K.t()) = (Q @ K.t())/sqrt(dk)\n",
    "# Masking -- shape(seq_len, seq_len)\n",
    "# then  Softmax(A), shape (seq_len, seq_len)\n",
    "# second_last => Softmax(A) @ V, shape (seq_len, dk)\n",
    "# lastly,  (Softmax(A) @ V) @ Wo, shape (seq_len, dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbec753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [0.72,0.45,0.31], # Dream\n",
    "    [0.75,0.20,0.55], # big\n",
    "    [0.30,0.80,0.40], # and\n",
    "    [0.85,0.35,0.60], # work\n",
    "    [0.55,0.15,0.75], # for\n",
    "    [0.25,0.20,0.85] # it\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37ef32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation of Scaled Dot Product Attention using Class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CausalAttentionSingleHead(nn.Module):\n",
    "    def __init__(self, dk, dmodel, dropout):\n",
    "        super(CausalAttentionSingleHead, self).__init__()\n",
    "        self.dk = dk\n",
    "        self.dmodel = dmodel\n",
    "        self.weight_query = nn.Linear(self.dmodel, self.dk)\n",
    "        self.weight_value = nn.Linear(self.dmodel, self.dk)\n",
    "        self.weight_key = nn.Linear(self.dmodel, self.dk)\n",
    "        ## defining Softmax\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        ## defining dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # calculating the Query, Key and Value tensors\n",
    "        Q = self.weight_query(X)\n",
    "        K = self.weight_key(X)\n",
    "        V = self.weight_value(X)\n",
    "        # calculating the attention score and scaling\n",
    "        attn_score = (Q @ K.t())/( self.dk**0.5)\n",
    "        # applying masking, first defining mask then applying\n",
    "        mask = torch.triu(torch.ones((X.shape[0], X.shape[0])), diagonal=1)\n",
    "        masked_attn_score = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # calculating the Softmax of the attn_score and adding dropouts\n",
    "        A = self.dropout(self.softmax(masked_attn_score))\n",
    "        # multiplying A with V\n",
    "        return A @ V, A\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58b39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = CausalAttentionSingleHead(dk = 2, dropout=0.2, dmodel= X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f10d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2055, 1.1526],\n",
       "         [0.2465, 1.2458],\n",
       "         [0.2133, 1.1259],\n",
       "         [0.1262, 0.6373],\n",
       "         [0.1801, 0.9241],\n",
       "         [0.1382, 0.6207]], grad_fn=<MmBackward0>),\n",
       " tensor([[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6336, 0.6164, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4398, 0.4349, 0.3753, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3227, 0.3166, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2613, 0.2496, 0.2490, 0.0000, 0.2349, 0.0000],\n",
       "         [0.2226, 0.0000, 0.0000, 0.2145, 0.0000, 0.1886]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd46dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec82888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9f180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436e3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor([\n",
    "    [0.72,0.45,0.31], # Dream\n",
    "    [0.75,0.20,0.55], # big\n",
    "    [0.30,0.80,0.40], # and\n",
    "    [0.85,0.35,0.60], # work\n",
    "    [0.55,0.15,0.75], # for\n",
    "    [0.25,0.20,0.85] # it\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7274d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.attention import CausalAttentionSingleHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbeca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attn = CausalAttentionSingleHead(dk = 512, dmodel = 3, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd56d800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2411, -0.9794,  1.2128,  ...,  0.5255, -0.1772, -0.1950],\n",
       "        [-0.1152, -0.4679,  0.5794,  ...,  0.2511, -0.0847, -0.0931],\n",
       "        [-0.3026, -0.9404,  1.1810,  ...,  0.6016, -0.1390, -0.2028],\n",
       "        [-0.1690, -0.4631,  0.6101,  ...,  0.3045, -0.0395, -0.1343],\n",
       "        [-0.2308, -1.0082,  1.1772,  ...,  0.6957, -0.0901, -0.1754],\n",
       "        [-0.2192, -0.7978,  0.9430,  ...,  0.5812, -0.0740, -0.1420]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c5688",
   "metadata": {},
   "source": [
    "## Masked MultiHead Attention Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc45823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, dmodel, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.h = h\n",
    "        self.dmodel = dmodel\n",
    "        assert self.dmodel % self.h == 0, \"head and dmodel configuration failed\"\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.weight_query = nn.Linear(dmodel, dmodel)\n",
    "        self.weight_key = nn.Linear(dmodel, dmodel)\n",
    "        self.weight_value = nn.Linear(dmodel, dmodel)\n",
    "        self.w_o = nn.Linear(dmodel, dmodel)\n",
    "        # softmax\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        # drop out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        seq_len = X.shape[0]\n",
    "        Q = self.weight_query(X)\n",
    "        K = self.weight_key(X)\n",
    "        V = self.weight_value(X)\n",
    "        Q_head = torch.permute(torch.reshape(Q, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        K_head = torch.permute(torch.reshape(K, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        V_head = torch.permute(torch.reshape(V, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        # calculating attention score and scaling\n",
    "        attn_score = (torch.matmul(Q_head, K_head.transpose(-1,-2)))/(math.sqrt(self.dk))\n",
    "        # applying masking\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        attn_score_masked = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # applying softmax\n",
    "        attn_weights = self.softmax(attn_score_masked)\n",
    "        # applying dropout\n",
    "        attn_weights_with_dropout = self.dropout(attn_weights)\n",
    "        # multiplying with V\n",
    "        A = torch.matmul(attn_weights_with_dropout, V_head)\n",
    "        # concatenation of the vector\n",
    "        concate_heads = torch.reshape(torch.permute(A, dims = (1, 0, 2)), shape = (seq_len, self.dmodel))\n",
    "        return self.w_o(concate_heads)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a791ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MHA = MultiHeadAttention(h = 4, dmodel= 512, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9210d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 10\n",
    "features = 512\n",
    "\n",
    "X = torch.randn(sentences, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d99846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ff3574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA.forward(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f788ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba9dcfe",
   "metadata": {},
   "source": [
    "## Transformer Block Complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a85937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.heads == 0, \"Choose Correct Head Number\"\n",
    "        self.dk = self.d_model // self.heads\n",
    "        # defining dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # defining softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # query_weight, key_weight, value_weight\n",
    "        self.weight_query = nn.Linear(self.d_model, self.d_model)\n",
    "        self.weight_key = nn.Linear(self.d_model, self.d_model)\n",
    "        self.weight_value = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
    "        self.ffn1 = nn.Linear(self.d_model, self.d_model*4)\n",
    "        self.ffn2 = nn.Linear(self.d_model*4, self.d_model)\n",
    "        self.layer_norm_1 = nn.LayerNorm(self.d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # extract shape of seq_len\n",
    "        seq_len = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        ##layer normalization\n",
    "        layer_norm1_output = self.layer_norm_1(X)\n",
    "\n",
    "        #project X (B, SEQ_LEN, Dmodel) into (B, Dmodel, Dmodel)\n",
    "        Q = self.weight_query(layer_norm1_output)\n",
    "        K = self.weight_key(layer_norm1_output)\n",
    "        V = self.weight_value(layer_norm1_output)\n",
    "        Q_heads = torch.permute(torch.reshape(Q, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "        K_heads = torch.permute(torch.reshape(K, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "        V_heads = torch.permute(torch.reshape(V, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "\n",
    "        # calculate attn score and scale\n",
    "        attn_score = (torch.matmul(Q_heads, K_heads.transpose(-1, -2)))/(math.sqrt(self.dk))\n",
    "\n",
    "        # masking \n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # applying mask\n",
    "        attn_score_with_mask = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # applying softmax\n",
    "        attn_weight_with_mask = self.softmax(attn_score_with_mask)\n",
    "        # apply dropouts \n",
    "        attn_wgts_mask_drpt = self.dropout(attn_weight_with_mask)\n",
    "\n",
    "        ### multiply with V_heads\n",
    "        A = torch.matmul(attn_wgts_mask_drpt, V_heads)\n",
    "        ## concatenation\n",
    "        concat_A = torch.reshape(torch.permute(A, dims = (0,2,1,3)), shape = (batch_size, seq_len, self.d_model))\n",
    "        output_masked_attn = self.w_o(concat_A)\n",
    "        #return layer_norm1_output\n",
    "\n",
    "        # residual connection 1\n",
    "        residual_connection_1 = X + output_masked_attn\n",
    "\n",
    "        #layer norm 2\n",
    "        layer_norm_2_output =self.layer_norm_2(residual_connection_1)\n",
    "        # ffnn\n",
    "        ffn1 = self.ffn1(layer_norm_2_output)\n",
    "        # activation\n",
    "        activation = nn.ReLU(ffn1)\n",
    "        # final layer\n",
    "        ffn2 = self.ffn2(activation)\n",
    "        \n",
    "        # residual connection 2\n",
    "        final_output = layer_norm_2_output + ffn2\n",
    "\n",
    "        #return the fnal vector\n",
    "        return final_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1f2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = TransformerBlock(heads = 8, d_model=512, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bdf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(32, 20, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecdb4baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7894,  1.9269, -1.8423,  ...,  0.3329, -0.7462, -1.6045],\n",
       "         [ 0.4369,  0.0260, -0.4353,  ...,  1.3975,  0.0652, -0.9650],\n",
       "         [ 0.3732,  0.3503,  0.5452,  ..., -1.3787, -1.0930,  0.3708],\n",
       "         ...,\n",
       "         [-0.7461, -0.4691, -0.3380,  ...,  0.7276, -0.5263, -0.4840],\n",
       "         [ 1.5050,  0.4982,  0.6430,  ...,  0.9810,  0.0398, -0.5948],\n",
       "         [ 1.8314,  1.3456,  0.1464,  ...,  0.9975, -1.4653,  0.9642]],\n",
       "\n",
       "        [[-0.6528,  1.3612, -0.4787,  ...,  0.8576,  0.5415, -1.0688],\n",
       "         [ 1.9451, -0.7653,  0.4269,  ...,  1.1427, -0.3564,  1.8415],\n",
       "         [ 0.3862,  0.2472,  0.3377,  ...,  1.1954, -1.1006,  0.4049],\n",
       "         ...,\n",
       "         [ 0.5666,  0.6158, -0.5619,  ...,  0.4627,  0.8919,  0.9075],\n",
       "         [-0.6789, -0.1943, -1.0082,  ..., -0.0751,  1.0986, -0.3521],\n",
       "         [ 0.8874, -0.3992,  0.5513,  ..., -1.4385,  0.1640, -0.3817]],\n",
       "\n",
       "        [[ 1.5878,  0.7143,  1.7367,  ...,  0.8599, -0.2548, -0.3510],\n",
       "         [ 0.4049, -0.4799,  0.0255,  ..., -1.4169, -0.0163, -0.6808],\n",
       "         [ 0.1773,  1.4634,  1.0995,  ...,  0.0056, -1.7877,  0.2434],\n",
       "         ...,\n",
       "         [ 0.9338, -0.2659, -0.2223,  ...,  1.1725,  0.2346,  0.3560],\n",
       "         [ 0.4167,  1.2282, -0.8436,  ...,  0.0512,  1.5116,  0.6267],\n",
       "         [-0.1021, -0.9534,  0.5770,  ...,  0.9017, -1.3126,  0.9365]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1426, -0.5348, -1.6838,  ..., -1.4516,  0.5587,  1.1567],\n",
       "         [-0.5575,  1.7888,  2.4817,  ...,  0.6195,  0.3673,  1.7387],\n",
       "         [-0.1247, -1.1863, -0.4723,  ..., -0.6367, -0.7597, -0.3330],\n",
       "         ...,\n",
       "         [ 0.4319, -0.5772, -0.4970,  ...,  0.6325,  0.8755,  0.1346],\n",
       "         [-0.9069,  2.9811,  0.3444,  ..., -1.2126,  0.0890,  1.7802],\n",
       "         [ 0.5992, -0.1918,  0.7679,  ..., -1.8207,  0.4405,  0.0828]],\n",
       "\n",
       "        [[-1.2911, -0.3525, -2.2582,  ..., -0.7074,  0.1513,  1.3925],\n",
       "         [-1.3190, -0.1382,  0.5087,  ...,  0.5641, -2.6467,  1.0554],\n",
       "         [-0.4393,  0.1045, -0.8486,  ..., -0.1603, -0.1597,  1.1484],\n",
       "         ...,\n",
       "         [-0.7285, -0.8976, -0.7668,  ...,  0.0820, -0.7338, -0.3004],\n",
       "         [-0.1005, -0.1528, -1.2423,  ..., -1.2053,  0.7736, -1.8199],\n",
       "         [ 2.1499, -0.8707,  0.5698,  ...,  0.6374, -0.4160,  0.4290]],\n",
       "\n",
       "        [[-0.3663,  0.1962,  0.4354,  ..., -2.2350,  1.8801,  0.1686],\n",
       "         [-0.3101, -0.3851, -1.6988,  ...,  1.2666, -0.0299, -0.4260],\n",
       "         [ 1.7962,  0.9126, -0.5298,  ..., -0.6891, -1.2072,  1.9209],\n",
       "         ...,\n",
       "         [ 0.7573,  0.1463,  0.1536,  ...,  0.5307, -0.2574, -0.3467],\n",
       "         [-1.5529, -0.3759,  0.0990,  ..., -3.3874,  1.9842, -0.0838],\n",
       "         [-0.5056,  0.0442,  0.1802,  ...,  0.2109,  0.9291, -1.1985]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d784cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class for ffnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    \"Implements Point Wise Feedforward Neural Network\"\n",
    "    def __init__(self, d_model, dffn, dropout):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #defining layers\n",
    "        self.w_1 = nn.Linear(d_model, dffn)\n",
    "        self.w_2 = nn.Linear(dffn, d_model)\n",
    "\n",
    "        # defining forward function\n",
    "        def forward(self, X):\n",
    "            return self.dropout(self.w_2_2(nn.ReLU(self.w_1(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae4d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1093dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
