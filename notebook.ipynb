{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3352c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6f7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters to define first\n",
    "\n",
    "# X is an input matrix, having shape (seq_len, dmodel)\n",
    "# Wq is query trainable, having shape (dmodel, dk)\n",
    "# Wk is key trainable, having shape (dmodel, dk)\n",
    "# Wv is value trainable, having shape (dmodel, dk), dv = dk\n",
    "# Wo trainable, having shape (dk*h, dmodel)\n",
    "# Q = X @ Wq, shape (seq_len, dk)\n",
    "# K = X @ Wk, shape (seq_len, dk)\n",
    "# V  = X @ Wv, shape (seq_len, dk), dk= dv\n",
    "# A = Attention(Q, K.t()), shape (seq_len, seq_len) where Attention(Q, K.t()) = (Q @ K.t())/sqrt(dk)\n",
    "# Masking -- shape(seq_len, seq_len)\n",
    "# then  Softmax(A), shape (seq_len, seq_len)\n",
    "# second_last => Softmax(A) @ V, shape (seq_len, dk)\n",
    "# lastly,  (Softmax(A) @ V) @ Wo, shape (seq_len, dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbec753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [0.72,0.45,0.31], # Dream\n",
    "    [0.75,0.20,0.55], # big\n",
    "    [0.30,0.80,0.40], # and\n",
    "    [0.85,0.35,0.60], # work\n",
    "    [0.55,0.15,0.75], # for\n",
    "    [0.25,0.20,0.85] # it\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37ef32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation of Scaled Dot Product Attention using Class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CausalAttentionSingleHead(nn.Module):\n",
    "    def __init__(self, dk, dmodel, dropout):\n",
    "        super(CausalAttentionSingleHead, self).__init__()\n",
    "        self.dk = dk\n",
    "        self.dmodel = dmodel\n",
    "        self.weight_query = nn.Linear(self.dmodel, self.dk)\n",
    "        self.weight_value = nn.Linear(self.dmodel, self.dk)\n",
    "        self.weight_key = nn.Linear(self.dmodel, self.dk)\n",
    "        ## defining Softmax\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        ## defining dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # calculating the Query, Key and Value tensors\n",
    "        Q = self.weight_query(X)\n",
    "        K = self.weight_key(X)\n",
    "        V = self.weight_value(X)\n",
    "        # calculating the attention score and scaling\n",
    "        attn_score = (Q @ K.t())/( self.dk**0.5)\n",
    "        # applying masking, first defining mask then applying\n",
    "        mask = torch.triu(torch.ones((X.shape[0], X.shape[0])), diagonal=1)\n",
    "        masked_attn_score = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # calculating the Softmax of the attn_score and adding dropouts\n",
    "        A = self.dropout(self.softmax(masked_attn_score))\n",
    "        # multiplying A with V\n",
    "        return A @ V, A\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58b39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = CausalAttentionSingleHead(dk = 2, dropout=0.2, dmodel= X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f10d168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2055, 1.1526],\n",
       "         [0.2465, 1.2458],\n",
       "         [0.2133, 1.1259],\n",
       "         [0.1262, 0.6373],\n",
       "         [0.1801, 0.9241],\n",
       "         [0.1382, 0.6207]], grad_fn=<MmBackward0>),\n",
       " tensor([[1.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6336, 0.6164, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4398, 0.4349, 0.3753, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3227, 0.3166, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2613, 0.2496, 0.2490, 0.0000, 0.2349, 0.0000],\n",
       "         [0.2226, 0.0000, 0.0000, 0.2145, 0.0000, 0.1886]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd46dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec82888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9f180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436e3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor([\n",
    "    [0.72,0.45,0.31], # Dream\n",
    "    [0.75,0.20,0.55], # big\n",
    "    [0.30,0.80,0.40], # and\n",
    "    [0.85,0.35,0.60], # work\n",
    "    [0.55,0.15,0.75], # for\n",
    "    [0.25,0.20,0.85] # it\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7274d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.attention import CausalAttentionSingleHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbeca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attn = CausalAttentionSingleHead(dk = 512, dmodel = 3, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd56d800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2411, -0.9794,  1.2128,  ...,  0.5255, -0.1772, -0.1950],\n",
       "        [-0.1152, -0.4679,  0.5794,  ...,  0.2511, -0.0847, -0.0931],\n",
       "        [-0.3026, -0.9404,  1.1810,  ...,  0.6016, -0.1390, -0.2028],\n",
       "        [-0.1690, -0.4631,  0.6101,  ...,  0.3045, -0.0395, -0.1343],\n",
       "        [-0.2308, -1.0082,  1.1772,  ...,  0.6957, -0.0901, -0.1754],\n",
       "        [-0.2192, -0.7978,  0.9430,  ...,  0.5812, -0.0740, -0.1420]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c5688",
   "metadata": {},
   "source": [
    "## Masked MultiHead Attention Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc45823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133b509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, dmodel, dropout):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.h = h\n",
    "        self.dmodel = dmodel\n",
    "        assert self.dmodel % self.h == 0, \"head and dmodel configuration failed\"\n",
    "        self.dk = self.dmodel // self.h\n",
    "        self.weight_query = nn.Linear(dmodel, dmodel)\n",
    "        self.weight_key = nn.Linear(dmodel, dmodel)\n",
    "        self.weight_value = nn.Linear(dmodel, dmodel)\n",
    "        self.w_o = nn.Linear(dmodel, dmodel)\n",
    "        # softmax\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        # drop out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        seq_len = X.shape[0]\n",
    "        Q = self.weight_query(X)\n",
    "        K = self.weight_key(X)\n",
    "        V = self.weight_value(X)\n",
    "        Q_head = torch.permute(torch.reshape(Q, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        K_head = torch.permute(torch.reshape(K, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        V_head = torch.permute(torch.reshape(V, shape=(seq_len, self.h, self.dk)), dims= (1, 0, 2))\n",
    "        # calculating attention score and scaling\n",
    "        attn_score = (torch.matmul(Q_head, K_head.transpose(-1,-2)))/(math.sqrt(self.dk))\n",
    "        # applying masking\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        attn_score_masked = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # applying softmax\n",
    "        attn_weights = self.softmax(attn_score_masked)\n",
    "        # applying dropout\n",
    "        attn_weights_with_dropout = self.dropout(attn_weights)\n",
    "        # multiplying with V\n",
    "        A = torch.matmul(attn_weights_with_dropout, V_head)\n",
    "        # concatenation of the vector\n",
    "        concate_heads = torch.reshape(torch.permute(A, dims = (1, 0, 2)), shape = (seq_len, self.dmodel))\n",
    "        return self.w_o(concate_heads)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a791ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MHA = MultiHeadAttention(h = 4, dmodel= 512, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9210d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 10\n",
    "features = 512\n",
    "\n",
    "X = torch.randn(sentences, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d99846f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ff3574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA.forward(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f788ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cba9dcfe",
   "metadata": {},
   "source": [
    "## Transformer Block Complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a85937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97d6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.heads == 0, \"Choose Correct Head Number\"\n",
    "        self.dk = self.d_model // self.heads\n",
    "        # defining dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # defining softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # query_weight, key_weight, value_weight\n",
    "        self.weight_query = nn.Linear(self.d_model, self.d_model)\n",
    "        self.weight_key = nn.Linear(self.d_model, self.d_model)\n",
    "        self.weight_value = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model)\n",
    "        self.ffn1 = nn.Linear(self.d_model, self.d_model*4)\n",
    "        self.ffn2 = nn.Linear(self.d_model*4, self.d_model)\n",
    "        self.layer_norm_1 = nn.LayerNorm(self.d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(self.d_model)\n",
    "        # define activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # extract shape of seq_len\n",
    "        seq_len = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        ##layer normalization\n",
    "        layer_norm1_output = self.layer_norm_1(X)\n",
    "\n",
    "        #project X (B, SEQ_LEN, Dmodel) into (B, Dmodel, Dmodel)\n",
    "        Q = self.weight_query(layer_norm1_output)\n",
    "        K = self.weight_key(layer_norm1_output)\n",
    "        V = self.weight_value(layer_norm1_output)\n",
    "        Q_heads = torch.permute(torch.reshape(Q, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "        K_heads = torch.permute(torch.reshape(K, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "        V_heads = torch.permute(torch.reshape(V, shape = (batch_size, seq_len, self.heads, self.dk)), dims = (0, 2, 1,3))\n",
    "\n",
    "        # calculate attn score and scale\n",
    "        attn_score = (torch.matmul(Q_heads, K_heads.transpose(-1, -2)))/(math.sqrt(self.dk))\n",
    "\n",
    "        # masking \n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # applying mask\n",
    "        attn_score_with_mask = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "        # applying softmax\n",
    "        attn_weight_with_mask = self.softmax(attn_score_with_mask)\n",
    "        # apply dropouts \n",
    "        attn_wgts_mask_drpt = self.dropout(attn_weight_with_mask)\n",
    "\n",
    "        ### multiply with V_heads\n",
    "        A = torch.matmul(attn_wgts_mask_drpt, V_heads)\n",
    "        ## concatenation\n",
    "        concat_A = torch.reshape(torch.permute(A, dims = (0,2,1,3)), shape = (batch_size, seq_len, self.d_model))\n",
    "        output_masked_attn = self.w_o(concat_A)\n",
    "        #return layer_norm1_output\n",
    "\n",
    "        # residual connection 1\n",
    "        residual_connection_1 = X + output_masked_attn\n",
    "\n",
    "        #layer norm 2\n",
    "        layer_norm_2_output =self.layer_norm_2(residual_connection_1)\n",
    "        # ffnn\n",
    "        ffn1 = self.ffn1(layer_norm_2_output)\n",
    "        # activation\n",
    "        activation = self.relu(ffn1)\n",
    "        # final layer\n",
    "        ffn2 = self.ffn2(activation)\n",
    "        \n",
    "        # residual connection 2\n",
    "        final_output = layer_norm_2_output + ffn2\n",
    "\n",
    "        #return the fnal vector\n",
    "        return final_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1f2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = TransformerBlock(heads = 8, d_model=512, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bdf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(32, 20, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdb4baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.5563e-02,  1.4333e+00, -1.6501e+00,  ...,  1.3692e+00,\n",
       "           1.7790e+00,  1.5419e-01],\n",
       "         [-1.2688e+00,  1.3711e+00, -1.1390e+00,  ..., -6.4300e-01,\n",
       "          -8.2388e-01,  3.6798e-01],\n",
       "         [-1.3032e+00, -1.1401e-02,  2.5159e-01,  ...,  1.4954e+00,\n",
       "           7.1369e-01,  9.5313e-03],\n",
       "         ...,\n",
       "         [ 1.8574e-01,  1.4225e+00,  1.0207e+00,  ...,  1.0022e+00,\n",
       "          -5.7860e-02, -3.5444e-02],\n",
       "         [-2.4432e+00, -1.0321e+00,  8.7815e-01,  ...,  1.4494e+00,\n",
       "           3.3746e-01,  6.7837e-01],\n",
       "         [-4.7686e-03, -1.8188e-01,  9.3257e-02,  ..., -6.7405e-01,\n",
       "          -4.3429e-03, -1.0248e+00]],\n",
       "\n",
       "        [[ 4.3783e-01,  2.6260e-01,  5.4969e-01,  ..., -9.9060e-01,\n",
       "          -6.9231e-01,  7.0725e-01],\n",
       "         [-3.2725e-01, -1.5383e+00,  3.0135e-01,  ...,  8.6273e-01,\n",
       "           4.0771e-02, -2.9588e+00],\n",
       "         [ 9.5258e-01,  8.3034e-01,  1.3435e+00,  ...,  7.9139e-01,\n",
       "           1.7850e+00,  7.1305e-01],\n",
       "         ...,\n",
       "         [-5.2161e-02,  9.8660e-01, -2.2269e+00,  ..., -1.0302e+00,\n",
       "           2.6950e-01, -6.6603e-01],\n",
       "         [ 3.9440e-01, -4.3875e-01, -3.9013e-01,  ..., -1.2704e-01,\n",
       "          -8.3617e-01, -1.2088e+00],\n",
       "         [-1.8621e+00, -1.1688e+00, -4.7575e-01,  ..., -4.9853e-01,\n",
       "          -1.4002e+00, -9.1976e-01]],\n",
       "\n",
       "        [[ 6.6064e-01, -1.7459e+00,  1.6372e-01,  ...,  1.9023e-01,\n",
       "          -1.2000e-01, -1.1012e-01],\n",
       "         [-1.4458e-01, -1.3441e+00,  5.1436e-01,  ...,  3.2521e-01,\n",
       "          -6.5598e-01, -5.7353e-01],\n",
       "         [ 9.5070e-01, -5.7671e-01,  4.5478e-01,  ...,  1.7323e-02,\n",
       "          -1.4117e+00,  1.7584e-01],\n",
       "         ...,\n",
       "         [ 1.3885e-01, -1.4907e+00, -3.4487e-01,  ...,  4.9726e-01,\n",
       "           1.6771e-01, -4.2660e-01],\n",
       "         [-7.5646e-01,  6.9899e-01, -8.3214e-01,  ..., -2.0634e+00,\n",
       "           3.5923e-01,  1.3550e+00],\n",
       "         [ 1.0556e-01, -9.1636e-01, -1.7952e+00,  ...,  1.4558e+00,\n",
       "          -1.1968e-01,  6.9151e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.8335e-01, -8.1643e-01, -6.1507e-01,  ...,  1.4965e+00,\n",
       "           1.0652e+00,  1.9535e+00],\n",
       "         [-1.1373e+00, -6.0422e-01, -1.2823e+00,  ..., -1.2865e-01,\n",
       "          -2.9828e-01,  1.8422e+00],\n",
       "         [ 3.1053e+00,  9.7492e-01,  1.2814e+00,  ..., -4.6760e-01,\n",
       "          -1.4909e+00, -1.0995e+00],\n",
       "         ...,\n",
       "         [ 7.4293e-01, -2.3032e-01, -1.5748e+00,  ...,  6.1300e-02,\n",
       "          -1.2100e+00,  1.0611e+00],\n",
       "         [ 4.4842e-01, -1.8085e+00, -3.1574e-01,  ...,  6.2000e-01,\n",
       "          -6.5217e-01,  1.1461e+00],\n",
       "         [-7.0200e-01,  1.1006e+00,  1.5310e-01,  ...,  2.3344e-01,\n",
       "           1.0952e+00,  9.5472e-01]],\n",
       "\n",
       "        [[ 7.5109e-01, -1.2080e+00, -8.1438e-01,  ..., -5.0054e-01,\n",
       "           4.7830e-01, -8.9267e-01],\n",
       "         [ 1.2930e+00,  4.3636e-01, -1.9837e-01,  ..., -5.7725e-01,\n",
       "          -5.1460e-01, -1.8727e+00],\n",
       "         [ 1.1331e+00,  1.2783e+00,  1.2241e+00,  ..., -1.4860e+00,\n",
       "           5.8611e-01, -1.9487e-01],\n",
       "         ...,\n",
       "         [ 2.6439e-01,  2.0200e-01, -8.1861e-01,  ..., -1.6783e+00,\n",
       "           3.6562e-01,  1.4502e+00],\n",
       "         [ 1.1394e+00,  1.4551e-01, -1.8180e+00,  ..., -1.3537e+00,\n",
       "          -4.9099e-01, -1.9959e+00],\n",
       "         [ 3.7422e-01, -1.9400e+00,  8.6579e-03,  ...,  1.8893e-01,\n",
       "           2.2329e-01, -2.1684e+00]],\n",
       "\n",
       "        [[-1.4544e+00,  8.3323e-01, -1.7581e+00,  ..., -2.9925e-01,\n",
       "          -1.8063e+00, -1.7646e-02],\n",
       "         [-5.6129e-01,  4.0189e-01, -4.8069e-01,  ..., -9.7938e-01,\n",
       "          -7.8235e-01, -4.6440e-01],\n",
       "         [ 1.7771e+00,  3.5662e-01, -7.3130e-01,  ..., -7.3142e-01,\n",
       "          -2.6558e-01, -4.1805e-01],\n",
       "         ...,\n",
       "         [ 4.8742e-01, -8.4062e-01, -1.2605e+00,  ..., -4.1800e-01,\n",
       "          -5.0364e-01, -6.4641e-02],\n",
       "         [-4.0531e-01, -2.0790e-02,  6.6630e-01,  ..., -1.4454e+00,\n",
       "          -1.0637e+00,  2.7410e+00],\n",
       "         [ 3.7930e-02,  2.6973e-03, -7.9383e-01,  ...,  1.2744e-01,\n",
       "           4.8387e-01,  8.6824e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d784cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class for PointWIseFeedForward Neural Network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    \"Implements Point Wise Feedforward Neural Network\"\n",
    "    def __init__(self, d_model, dffn, dropout):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #defining layers\n",
    "        self.w_1 = nn.Linear(d_model, dffn)\n",
    "        self.w_2 = nn.Linear(dffn, d_model)\n",
    "\n",
    "        # defining forward function\n",
    "        def forward(self, X):\n",
    "            return self.dropout(self.w_2_2(nn.ReLU(self.w_1(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf54805",
   "metadata": {},
   "source": [
    "## Positional Encoding Implementation -  sin, cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afae4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "def sin(pos, i):\n",
    "        return math.sin(pos/10000**(2*i/d_model))\n",
    "\n",
    "def cos(pos, i):\n",
    "        return math.cos(pos/10000**(2*i/d_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e517fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "random_embeddings = torch.randn(seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9919af70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [-9.6140e-01, -2.7516e-01, -6.3753e-01,  ...,  1.0000e+00,\n",
       "          1.7623e-03,  1.0000e+00],\n",
       "        [-7.5099e-01,  6.6032e-01, -9.9638e-01,  ...,  1.0000e+00,\n",
       "          1.8659e-03,  1.0000e+00],\n",
       "        [ 1.4988e-01,  9.8870e-01, -4.9773e-01,  ...,  1.0000e+00,\n",
       "          1.9696e-03,  1.0000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postional_encoding_vector = []\n",
    "\n",
    "for pos in range(seq_len):\n",
    "    temp = []\n",
    "    for i in range(d_model//2):\n",
    "            temp.append(sin(pos,i))\n",
    "            temp.append(cos(pos,i))\n",
    "\n",
    "    postional_encoding_vector.append(temp)\n",
    "\n",
    "postional_encoding_vector = torch.tensor(postional_encoding_vector)\n",
    "\n",
    "postional_encoding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional = [[sin(pos,i), cos(pos,i)] for i in range(d_model//2) for pos in range(seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "514057db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(positional).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10288a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement positional encoding using torch\n",
    "\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def sin(self, pos, i):\n",
    "        return math.sin(pos/10000**(2*i/self.d_model))\n",
    "    \n",
    "    def cos(self, pos, i):\n",
    "        return math.cos(pos/10000**(2*i/self.d_model))\n",
    "    \n",
    "    def calculate_pe(self):\n",
    "        positional_encoding_vector = []\n",
    "        for pos in range(self.seq_len):\n",
    "            temp = []\n",
    "            for i in range(self.d_model//2):\n",
    "                    temp.append(sin(pos,i))\n",
    "                    temp.append(cos(pos,i))\n",
    "            positional_encoding_vector.append(temp)\n",
    "        return torch.tensor(positional_encoding_vector)\n",
    "        \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f93346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.positional_encoding import PositionalEncoding\n",
    "\n",
    "pe = PositionalEncoding(seq_len=1024, d_model=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422eabf",
   "metadata": {},
   "source": [
    "## Decoder full Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fa82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## class of full transfprmer block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from model.positional_encoding import PositionalEncoding\n",
    "from model.transformer_block import TransformerBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8dcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout, blocks, vocab_size):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layernorm = nn.LayerNorm(self.d_model)\n",
    "        self.linear = nn.Linear(self.d_model, self.vocab_size)\n",
    "        self.dropout = dropout\n",
    "        self.blocks = blocks\n",
    "        # defining transformer blocks\n",
    "        self.blocks_list = nn.ModuleList()\n",
    "        for i in range(self.blocks):\n",
    "            self.blocks_list.append(TransformerBlock(heads = self.h, d_model= d_model, dropout=self.dropout))\n",
    "        # positional encoding \n",
    "        self.positional_encoding = PositionalEncoding(d_model = self.d_model)\n",
    "\n",
    "\n",
    "    # forward, assuming X (B, Seq_len, d_model)\n",
    "    def forward(self, X):\n",
    "        seq_len = X.shape[1]\n",
    "        d_model = X.shape[-1]\n",
    "        pe = self.positional_encoding.calculate_pe(X)\n",
    "        # modifying input tensor by adding X + positional encoding\n",
    "        input_tensor = X + pe\n",
    "        # this input will go into 6 transformer blocks\n",
    "        ## here also I will need to put them automatically\n",
    "        x = input_tensor.clone()\n",
    "        for block in self.blocks_list:\n",
    "            y = block(x)\n",
    "            x = y        \n",
    "        # adding layer norm 1 \n",
    "        layer_norm_1 = self.layernorm(x)\n",
    "\n",
    "        # adding linear layer\n",
    "        linear_layer_output = self.linear(layer_norm_1)\n",
    "        # returning final layer output\n",
    "        return linear_layer_output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0990a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(d_model= 512, h = 8, dropout=0.1, vocab_size=50247, blocks = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd1161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(32, 20, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44ef934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 50247])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.forward(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c592d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decoder import TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d1e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "decdr = TransformerDecoder(d_model=512, h = 8, dropout=0.2, blocks=6,vocab_size=50247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3c7196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.randn(64, 121, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d360d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 121, 50247])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decdr.forward(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4f84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3468e101",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52204e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f24a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50247\n",
    "d_model = 512\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50219b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([[1, 2, 4], [0, 9, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06d4acd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8605dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "086aafb6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595c9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9922589",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_tokenizer = CharTokenizer(file = r\"C:\\Users\\amanm\\Desktop\\learning\\transformer_from_scratch\\data\\tiny-shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accf3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_list = character_tokenizer.encode(file_path= \"testing.txt\")[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cfd16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 63, 1, 52, 39]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a203c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = character_tokenizer.decode(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef6a778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My na'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6664e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6fc3c8",
   "metadata": {},
   "source": [
    "## Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24504c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.dataset import CustomTextData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148acab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = CustomTextData(file_path= r\"C:\\Users\\amanm\\Desktop\\learning\\transformer_from_scratch\\data\\tiny-shakespeare.txt\", seq_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a7a3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115374"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f923ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = custom_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080293c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0653668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
       "        56, 43])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f361f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016374bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\amanm\\Desktop\\learning\\transformer_from_scratch\\data\\tiny-shakespeare.txt\") as f:\n",
    "    sample = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d20196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = CharTokenizer(file = sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf056e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = sample[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4b43ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b635b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_obj.decode(tokenizer_obj.encode(x1)) == x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3d447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841ba75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
